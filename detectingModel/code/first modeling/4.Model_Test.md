# 학습된 model test

* 학습된 CNN 모델(`facemask.h5`)을 동영상을 이용하여 test 를 진행하였다.



* 밑의 이미지는 시스템 구조도이다.

![1차모델_시스템구조도](C:\Users\user\Desktop\1차모델_시스템구조도.png)



* 시스템 구조도의 `face_recognition` 단계에서 CNN 방식을 이용하여 얼굴을 검춣하였다.
  * CNN 기반의 `face_recognition.face_locations` 함수는 `GPU` 이용을 기본으로 만들어져 `CPU` 를 이용해서 진행하는 상황에서는 한 `frame` 당 최대 분단위의 작업 처리 시간이 걸렸다.
  * 동영상 1초는 20~30 개의 `frame` 으로 구성되어 위의 방식을 실시간 영상에 적용시키는 것을 어려움이 있었다.
    * `face_recognition.face_locations` 함수를 `HOG` 기반으로 이용하였을 때는 마스크 미착용 얼굴은 검출 되고, 마스크 착용 얼굴은 검출하지 못하였다. 하지만 검출 시간은 매우 빨라 실시간 적용이 가능하였다.
    * 따라서 마스크 미착용 얼굴은 `face_recognition.face_locations` 을 `HOG` 기반으로 이용하여 찾고, 마스크 착용 얼굴은 model 를 만들기로 하였다.(2차 모델링)



* header

```python
import cv2
from keras.models import load_model
from keras.preprocessing.image import img_to_array, load_img
import face_recognition
```



* 동영상에서 얼굴(마스크 착용 & 마스크 미착용) 검출 후 model 을 통해 마스크 착용여부 판단

```python
# video_file_path : test video path
# filename : model 적용된 video 이름
# percentage : 이미지의 축소 비율 / ex) 입력값 20 -> 이미지 20퍼센트 크기로 축소
# angle : 이미지 회전 각도
# way : face_recognition 이용 방식(Hog, CNN)
def detect_face_mask(video_file_path, filename, percentage, angle, way):
    # 동영상에 쓰일 글씨체
    font = cv2.FONT_HERSHEY_SIMPLEX
    # CNN model load
    model = load_model('lib/facemask.h5')

    #재생할 파일 
    VIDEO_FILE_PATH = video_file_path

    #저장할 파일 이름
    
    filename = filename

    # 동영상 파일 열기
    cap = cv2.VideoCapture(VIDEO_FILE_PATH)

    #잘 열렸는지 확인
    if cap.isOpened() == False:
        print ('Can\'t open the video (%d)' % (VIDEO_FILE_PATH))
        exit()

    titles = ['orig']
    #윈도우 생성 및 사이즈 변경
    for t in titles:
        cv2.namedWindow(t, cv2.WINDOW_NORMAL)
#         cv2.resizeWindow(t, 640, 360)

    #재생할 파일의 넓이 얻기
    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
    #재생할 파일의 높이 얻기
    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
    #재생할 파일의 프레임 레이트 얻기
    fps = cap.get(cv2.CAP_PROP_FPS)
    #재생할 파일의 프레임 레이트 얻기
    counts = cap.get(cv2.CAP_PROP_FRAME_COUNT)

    print('width {0}, height {1}, fps {2}, counts {3}'.format(width, height, fps, counts))

    #XVID가 제일 낫다고 함.
    #linux 계열 DIVX, XVID, MJPG, X264, WMV1, WMV2.
    #windows 계열 DIVX
    #저장할 비디오 코덱
    fourcc = cv2.VideoWriter_fourcc(*'DIVX')

    #파일 stream 생성
    out = cv2.VideoWriter(filename, fourcc, fps, (int(width), int(height)))
    #filename : 파일 이름
    #fourcc : 코덱
    #fps : 초당 프레임 수
    #width : 넓이
    #height : 높이

    while(True):
        #파일로 부터 이미지 얻기
        ret, frame = cap.read()
        #더 이상 이미지가 없으면 종료
        #재생 다 됨
        if frame is None:
            break
 
        reduce_percentage = percentage
        
        # 이미지 회전(동영상이 회전되어 있지 않으면 angle 값에 0 입력하여 진행)
        matrix = cv2.getRotationMatrix2D(( int(width)/2, int(height)/2 ), angle, 1)
        frame = cv2.warpAffine(frame, matrix, ( int(width), int(height) ))
        # 이미지 크기 조절
        img = cv2.resize(frame, dsize=(0, 0), fx=reduce_percentage/100, fy=reduce_percentage/100, interpolation=cv2.INTER_LINEAR)
        
        # face_recognition
        if way == 'CNN':
            face_locations = face_recognition.face_locations(img, model='cnn')
        elif way == 'HOG':
            face_locations = face_recognition.face_locations(img)
    
        if len(face_locations) > 0:
            for face_location in face_locations:
                top, right, bottom, left = face_location
                # 원본 이미지에 맞게 픽셀값 조정
                top = int(top * 100 / reduce_percentage)
                right = int(right * 100 / reduce_percentage)
                bottom = int(bottom * 100 / reduce_percentage)
                left = int(left * 100 / reduce_percentage)
                # 얼굴 부분만 trim
                trim_img = frame[top:bottom, left:right]
                # model 이용하기 위해 저장 후 load_img 로 재 업로드 하여 예측 진행
                cv2.imwrite("test.jpg", trim_img)
                trim_img = load_img("test.jpg", target_size=(80, 80))
                trim_img = img_to_array(trim_img)
                trim_img = np.expand_dims(trim_img, axis=0)
                preds = model.predict(trim_img)
        
                if preds >= 0.5:
                    cv2.rectangle(frame, (left, top), (right,bottom), (255,0,0), 2)
                    cv2.putText(frame, 'Mask on', (left-5, top-5), font, 0.9, (255,0,0), 2)
                    print("Mask")
                else:
                    cv2.rectangle(frame, (left, top), (right,bottom), (0,0,255), 2)
                    cv2.putText(frame, 'No Mask', (left-5, top-5), font, 0.9, (0,0,255), 2)
                    print("Face")

        # 얼굴 인식된 이미지 화면 표시
        cv2.imshow(titles[0],frame)

        # 인식된 이미지 파일로 저장
        out.write(frame)

        #1ms 동안 키입력 대기
        if cv2.waitKey(1) == 27:
            break;


    #재생 파일 종료
    cap.release()
    #저장 파일 종료
    out.release()
    #윈도우 종료
    cv2.destroyAllWindows()
```

